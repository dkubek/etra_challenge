{
  "hash": "4dabb94d181a1ad0acab9e3363987476",
  "result": {
    "markdown": "---\ntitle: ETRA Challenge Report\nauthor: DÃ¡vid Kubek\ndate: last-modified\ndate-format: long\nbibliography: references.bib\nformat:\n  html:\n    toc: true\n    theme: custom.scss\n    code-fold: true\n    code-line-numbers: true\n  pdf:\n    echo: false\nexecute:\n  cache: true\n  freeze: true\n---\n\nIn 2019, the [ETRA](https://etra.acm.org/) organization announced a challenge\nfor the analysis of a dataset pertaining to human eye-movement. The objective of\nthe challenge was to utilize various tools to uncover intriguing relationships\nand information within the data, potentially leading to novel insights.\nAdditional details regarding the challenge can be found in the official\n[Challenge Track Call for Papers](https://etra.acm.org/2019/challenge.html)\nissued by ETRA.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-summary=\"Imports and setup\"}\nimport warnings \nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport pymc as pm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nfrom etra import read_data\n\n# Apply the default theme\nsns.set_theme()\n```\n:::\n\n\n# Getting data\n\nDownload zip file of the [dataset](http://smc.neuralcorrelate.com/ETRA2019/ETRA2019Challenge.zip). Description of the dataset and how\nthe data was collected can be found on [ETRA dataset description](https://etra.acm.org/2019/challenge.html).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-summary=\"Download data\"}\nfrom etra import ETRA\n\ndataset = ETRA()\n```\n:::\n\n\nThe directory ``data/`` contains the following directories/files:\n\n- ``data/`` : Contains subdirectories containing data of individual\n  participants. Each subdirectory is filled with CSV files, each containing 45\n  seconds of eye tracker data\n- ``images/`` : Contains the images shown to the participants for each task.\n- ``DataSummary.csv`` : Contains mouse clicks of the participants for tasks like\nPuzzle or Where's Waldo.\n\n# Hypotheses\n\nWe will be interested in the following hypotheses:\n\n1. **Is there a point where a participant becomes tired/bored when viewing a scene? (the frequency of saccades decreases) If so, are some scenes less interesting then others?**\n    \n    There are different types of scenes in the study and some might be more\n    visually stimulating then others. For example, it is not unreasonable to\n    expect that a participant will stay much more engaged in a scene with a lot\n    of detail to study then when looking at a blank scene. Another possibility\n    is that during visual searches (picture puzzles, _Where is Waldo_) a\n    participant may become frustrated and just give up on the task. \n\n    The reasoning behind the way we are going to measure this is as follows.\n    When a participant is engaged in a scene, we assume that their gaze is going\n    to dart across the scene rapidly, as they are studying all the details and\n    the number of saccades during a fixed time frame is going to be very high.\n    On the other hand, when subject becomes bored or frustrated, we might expect\n    that they are going to rest their gaze at some point and not move their eyes\n    very much (maybe just waiting for the trial to be over) and so the frequency\n    of saccades would decrease.\n\n    However, the decrease in frequency of saccades might not be exactly\n    an indicator of boredom or frustration. It is not unlikely that\n    the frequency of saccades is high every time a new scene is introduced and\n    naturally decreases as the subject becomes acquainted with the scene.\n\n    Whatever the reason might be, if there indeed is such a changing point in\n    the frequency of saccades, it might be interesting to study the time for\n    such change and it's relationship to participant, scene and task.\n\n\n2. **Does the pupil size change depending on scene? What about dependence on task?**\n\n    The motivation behind this hypothesis is that we would like to study the\n    excitement of a subject viewing some scene or performing some task. One of\n    the indicators of excitement might be the change in pupil size (i.e. we\n    expect pupil to dilate during excitement and contract otherwise).\n\n    There are various tasks and scenes and the level of excitement (pupil\n    dilation) might differ in each:\n\n    * in **picture puzzles** we might expect that the level of excitement is\n    kept high during the whole duration, peaking when subject finds a difference,\n\n    * in _**Where is Waldo?**_ puzzle we might expect the level of excitement\n    being low as the subject is having difficulties finding Waldo and peaking\n    when (if) Waldo is found,\n\n    * viewing a **blank scene** might not excite much.\n\n    Therefore, it is interesting to ask which tasks and scenes are the most\n    visually stimulating and whether any significant distinction can be made.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-summary=\"Choose an example subject\"}\nsubject_no = 22\n```\n:::\n\n\n# Hypothesis 1: Saccade Frequency\n\nWe will restrict ourselves only to the following hypothesis:\n \n * $H_0$ _(null hypothesis)_ : The frequency of saccades does not decrease when\n   viewing a scene.\n * $H_1$ _(alternative hypothesis)_ : The frequency of saccades decreases at some\n   point in the trial.\n\n## Load Data\n\nWe will restrict ourselves only to one subject. We have arbitrarily chosen the\nparticipant with ID ``022``. We select and load all _Free Viewing_ data with\n``Natural`` and ``Blank`` scenes. We will be interested in columns:\n\n* ``time`` : Indicating the time in milliseconds from the beggining of the trail.\n* ``x``, ``y`` : The position on the screen where the participant was looking at the given time.\n* ``rp``: Pupil size of the right eye.\n\nIn @tbl-hyp1-data we provide a sample of the data we will use.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-summary=\"Load data\"}\ndf_hyp1 = (dataset.data_dir / \"data\" / \"{0:0>3}\".format(subject_no))\\\n    .glob(\"*FreeViewing_*.csv\")\ndf_hyp1 = pd.concat((read_data(f) for f in df_hyp1)).sort_values(by=\"Time\")\n\ndf_hyp1 = df_hyp1.rename(\n    {\n        \"Time\": \"time\",\n        \"trial_id\": \"trial\",\n        \"LXpix\": \"x\",\n        \"LYpix\": \"y\",\n        \"RP\": 'rp'\n    },\n    axis=1\n)\ndf_hyp1[\"time\"] = df_hyp1.groupby([\"participant_id\", \"trial\"])[\"time\"]\\\n    .transform(lambda x: x - x.min())\ndf_hyp1 = df_hyp1[\n    [\n       'participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id',\n       'time', 'x', 'y', 'rp'\n    ]\n]\n```\n:::\n\n\n::: {#tbl-hyp1-data .cell tbl-cap='Sample of the data for hypothesis 1.' execution_count=6}\n``` {.python .cell-code code-summary=\"Print sample of hypothesis 1 data\"}\ndf_hyp1.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>trial</th>\n      <th>fv_fixation</th>\n      <th>task_type</th>\n      <th>stimulus_id</th>\n      <th>time</th>\n      <th>x</th>\n      <th>y</th>\n      <th>rp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>0</td>\n      <td>454.42</td>\n      <td>317.175</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>2</td>\n      <td>454.66</td>\n      <td>312.000</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>4</td>\n      <td>453.30</td>\n      <td>312.375</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>6</td>\n      <td>454.10</td>\n      <td>309.450</td>\n      <td>587</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>8</td>\n      <td>454.82</td>\n      <td>310.725</td>\n      <td>585</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Data Manipulation\n\nFirst step of the pre-processing will be to remove the blinks. We look at the\npupil size and consider every value smaller than some threshold to be an\nindication of a blink. We take the average pupil to be between 1 and 8 mm\n[@PupilDiameter]. We set 1 mm as our threshold. We will then discard a portion\nof our data around such points. The average blink duration ranges between 100\nand 400 ms [@BlinkDuration]. We delete 200 ms before and after each point that \nfalls below the threshold and another 200 ms before and after each\nblink/semi-blink to eliminate the initial and final parts in which the pupil was\nstill partially occluded.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-summary=\"Function for removing blinks\"}\ndef remove_blinks(data : pd.DataFrame, column='rp'):\n    ans = data.copy()\n    n_rows = ans.shape[0]\n    \n    lows = np.where(ans['rp'].lt(100))[0]\n    for low in lows:\n        ans.loc[max(0, low-40):min(low+40, n_rows), column] = pd.NA\n        \n    return ans.dropna()\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-summary=\"Plot pupil size with and without blinks\"}\nbefore = df_hyp1[df_hyp1.trial == '106'].assign(blinks_removed='No')\nafter = remove_blinks(before).assign(blinks_removed='Yes')\nhlp = pd.concat([before, after])\n\ng = sns.relplot(\n    data=hlp, \n    x='time', y='rp',\n    kind='line',\n    row='blinks_removed',\n    aspect=3\n)\ng.set_xlabels(\"Time (ms)\")\ng.set_ylabels(\"Pupil size\")\ng.axes[0, 0].set_title(\"Pupil size as measured\")\ng.axes[1, 0].set_title(\"Pupil size with blinks removed\");\n```\n\n::: {.cell-output .cell-output-display}\n![Pupil size data with blinks (top) and with blinks removed (bottom).](etra_challenge_files/figure-html/fig-blinks-removed-output-1.png){#fig-blinks-removed width=1424 height=944}\n:::\n:::\n\n\nFor each trial, we will detect saccades using the [REMoDNaV](https://github.com/psychoinformatics-de/remodnav) \npython library [@Dar2019]. This gives us the start and end times of saccades\n(and other events) as well as additional information such as start and end\nposition of the gaze or peak velocity. We will consider only the start times of\nthe saccades and disregard all other data. @tbl-natural-sacc-data lists a sample\nof the processed data.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-summary=\"Function for detecting saccades\"}\nfrom etra import detect\n\ndef detect_saccades_by_groups(data, groupby=[\"participant_id\", \"trial\"]):\n    ans = []\n    groups = data.groupby(groupby)\n    for (pid, trial), group in groups:\n        tmp = remove_blinks(group)\n        tmp = detect(group)\n        tmp = tmp[tmp[\"label\"] == \"SACC\"]\n        tmp[\"participant_id\"] = pid\n        tmp[\"trial\"] = trial\n        ans.append(tmp)\n    \n    return pd.concat(ans)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-summary=\"Preprocess data\"}\ndf_hyp1_natural = df_hyp1[df_hyp1.task_type == \"Natural\"]\ndf_hyp1_natural_sacc = detect_saccades_by_groups(\n    df_hyp1_natural\n)\n\ndf_hyp1_blank = df_hyp1[df_hyp1.task_type == \"Blank\"]\ndf_hyp1_blank_sacc = detect_saccades_by_groups(\n    df_hyp1_blank\n)\n\ntrial_info_natural = df_hyp1_natural[\n    ['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id']\n    ].drop_duplicates()\ndf_hyp1_natural_sacc = trial_info_natural.merge(df_hyp1_natural_sacc, on=[\n    \"participant_id\", \"trial\"], how=\"left\")\ndf_hyp1_natural_sacc = df_hyp1_natural_sacc[\n    [\n        'participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id', \n        'start_time',\n    ]\n]\n\ntrial_info_blank = df_hyp1_blank[\n    ['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id']\n].drop_duplicates()\ndf_hyp1_blank_sacc = trial_info_blank.merge(df_hyp1_blank_sacc, on=[\n    \"participant_id\", \"trial\"],\n    how=\"left\"\n)\ndf_hyp1_blank_sacc = df_hyp1_blank_sacc[\n    [\n        'participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id',\n        'start_time',\n    ]\n]\n```\n:::\n\n\n::: {#tbl-natural-sacc-data .cell tbl-cap='Sample of Freeviewing data.' execution_count=11}\n``` {.python .cell-code code-summary=\"Print sample of processed data\"}\ndf_hyp1_natural_sacc.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>trial</th>\n      <th>fv_fixation</th>\n      <th>task_type</th>\n      <th>stimulus_id</th>\n      <th>start_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>022</td>\n      <td>003</td>\n      <td>FreeViewing</td>\n      <td>Natural</td>\n      <td>nat010</td>\n      <td>258</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>022</td>\n      <td>003</td>\n      <td>FreeViewing</td>\n      <td>Natural</td>\n      <td>nat010</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>022</td>\n      <td>003</td>\n      <td>FreeViewing</td>\n      <td>Natural</td>\n      <td>nat010</td>\n      <td>670</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>022</td>\n      <td>003</td>\n      <td>FreeViewing</td>\n      <td>Natural</td>\n      <td>nat010</td>\n      <td>1166</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>022</td>\n      <td>003</td>\n      <td>FreeViewing</td>\n      <td>Natural</td>\n      <td>nat010</td>\n      <td>1266</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Methodology and Exploration\n\nTo help us understand how we should model the observations, we first look at the\ndistribution of saccades in time. The $x$ axis represents the time from the\nstart of a trial and each row is a sequence of dots representing the start time\nof a saccade.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-summary=\"Plot the distriution of saccades in time\"}\ng = sns.catplot(\n    data=df_hyp1_natural_sacc,\n    x='start_time',\n    y='stimulus_id',\n    jitter=False,\n    aspect=3,\n)\ng.set_xlabels(\"Start time of a saccade in milliseconds\")\ng.set_ylabels(\"ID of the stimulus\")\ng.set(title=\"Distribution of saccades in time\");\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of saccades in time in Natural scenes for one participant.](etra_challenge_files/figure-html/fig-saccades-in-time-output-1.png){#fig-saccades-in-time width=1424 height=483}\n:::\n:::\n\n\nAs the occurance of a saccade is a complex process that depends on the scene\nviewed, mental state of the participant, tiredness or maybe even things like\npersonal history, eyesight, mood and many other factors, it is impossible for us\nto accurately predict the occurance of the next saccade. To simplify the\nsituation, we might look at it as on a stochastic process, where for some small\nenough time interval, we have a fixed probability for an occurance of a saccade.\n\nLet $N_t$ be a variable counting the number of saccades in a time interval\n$[0,t]$.  Our assumption can be formulated as\n$$\n\\lim_{t \\to 0} \\Pr[N_t \\ge 1]/t = \\lambda\n$$\nfor some $\\lambda \\in \\mathbb{R}^+$. That is, the probability of an saccade in a\nsmall time interval $[0, t]$ tends to $\\lambda t$.\n\nDue to physical human limitations, we also have\n$$\n\\lim_{t \\to 0} \\Pr[N_t \\ge 2]/t = 0\n$$\nor said differently, for a small enough time interval, it is impossible to have\ntwo or more successive saccades.\n\nThe number of saccades in some interval $[t, s]$ can be expressed then as $N(t)\n- N(s)$. Additionally, we will assume that for any two disjoint time intervals\n$[t_1, t_2]$ and $[t_3, t_4]$, the distribution of $N(t_2) - N(t_1)$ is\nindependent of the distribution of $N(t_4) - N(t_3)$.\n \nWith these assumtions in place, we are justified to model the occurances of\nsaccades as a _Poisson process_ with rate $\\lambda$ [@mitzenmacher_upfal_2012].\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-summary=\"Function for counting the number of saccades each second\"}\ndef saccade_frequency_count_by_second(data):\n    \"\"\" Count the number of saccades in each second of the trial \"\"\"\n    \n    bins = (data.start_time // 1000).value_counts().sort_index()\n    data = pd.DataFrame({\"time\": range(45)})\n    data[\"sacc_count\"] = bins\n    data.fillna(0, inplace=True)\n    data.sacc_count = data.sacc_count.astype(int)\n    \n    return data\n```\n:::\n\n\nThe theory of Poisson processes then tells us that the number of saccades in\nsome time period of length $t$ is a Poisson random variable with expectation\n$\\lambda t$. We will divide the time of the experiment to seconds (so $t = 1$)\nand count the number of saccades in each second.  The @fig-saccade-dist\nillustrates such counts for one chosen trial.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-summary=\"Chose one scene as an example\"}\nstimulus = \"nat013\"\nscene = df_hyp1_natural_sacc[df_hyp1_natural_sacc.stimulus_id == stimulus]\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-summary=\"Plot the frequency of saccades in time\"}\nhlp = scene.copy()\nhlp.start_time = hlp.start_time.apply(lambda ms: ms // 1000)\n\ng = sns.displot(data=hlp, x='start_time', bins=45, stat=\"count\", aspect=3)\ng.set_xlabels(\"Time (s)\")\ng.set_ylabels(\"Number of saccades\")\ng.set(title=f\"Frequency of saccades\");\n```\n\n::: {.cell-output .cell-output-display}\n![Frequency of saccades in time for one chosen natural scene.](etra_challenge_files/figure-html/fig-saccade-dist-output-1.png){#fig-saccade-dist width=1424 height=483}\n:::\n:::\n\n\nWe are looking for a change in behaviour in our participants. To model it we\nwill assume that in some initial time interval the number of saccades will\nfollow some Poisson process with rate $\\lambda_1$ up until some time $\\tau$\nafter which _something_ changes and the saccades will follow a different process\nwith rate $\\lambda_2$.\n\nLet $S_t$ denote the count of saccades at time $t$. We have\n$$\n    S_t \\sim \\mathrm{Poisson}(\\lambda^{(t)})\n$$\nHowever, we do not know what the parameter $\\lambda^{(t)}$ is. It might be either\n$\\lambda_1$ or $\\lambda_2$\ndepending on where we put the changing point $\\tau$, so\n$$\n\\lambda^{(t)} \n= \\begin{cases}\n    \\lambda_1 & t < \\tau \\\\\n    \\lambda_2 & t \\ge \\tau\n\\end{cases}\n$$\n\nWe will try to infer these variables from our data using Bayesian statistics. We\nwill need to choose prior distributions to do that. For $\\lambda_1$ and\n$\\lambda_2$ we need to choose from continouous distribution that is able to\nobtain (potentially) all positive values. Exponential distribution\nsatisfies this requirement, so we set\n$$\n    \\lambda_1 \\sim \\mathrm{Exp}(\\alpha)\n$$\n$$\n    \\lambda_2 \\sim \\mathrm{Exp}(\\alpha)\n$$\n\nwhere $\\alpha$ is some hyper-parameter. As we have no prior knowledge about\n$\\tau$, we simply choose\n$$\n    \\tau \\sim \\mathrm{Unif}(0, 44)\n$$\n\nTo help with the inference, we set the hyperparameter $\\alpha$ such that\n$$\n    \\frac{1}{\\alpha} \n    = \\mathbb{E}[\\lambda \\mid \\alpha] \n    \\approx \\frac{1}{45} \\sum_{t = 0}^{44} S_t\n$$\nwhich is _about_ the value we would expect the $\\lambda$s to be distributed\naround.\n\n::: {.callout-note}\nThis approach was adapted from the first chapter of \n[Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers).\n:::\n\nFor our modelling we will use the [PyMC](https://www.pymc.io/welcome.html) \n[@Salvatier2016] implementation of Monte Carlo Markov Chains, using the NUTS\nsampler with default parameters to generate $5000$ samples from the posterior\ndistribution.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code code-summary=\"Define model for the saccade frequency\"}\ndef model_saccades(data):\n    # Get the count data for one chosen trial\n    count_data = saccade_frequency_count_by_second(data)\n    count_data = count_data.sacc_count\n    n_count_data = count_data.shape[0]\n    \n    with pm.Model() as model:\n        alpha = 1.0/count_data.mean()\n\n        lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n        lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n\n        tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data - 1)\n\n        index = np.arange(n_count_data)\n        lambda_ = pm.math.switch( tau >= index, lambda_1, lambda_2 )\n        \n        delta = pm.Deterministic(\"delta\", lambda_2 - lambda_1)\n\n        observation = pm.Poisson(\"obs\", lambda_, observed=count_data)\n        \n    return model\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-summary=\"Run the chain\"}\nmodel = model_saccades(scene)\nwith model:\n    trace = pm.sample(5_000)\n```\n:::\n\n\n@fig-example-chain-posterior shows the posterior distributions of the\nparameters $\\tau$, $\\lambda_1$, $\\lambda_2$ and $\\Delta = \\lambda_2 - \\lambda_1$,\nwhile @tbl-example-chain-summary provides summary statistics from the chain.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-summary=\"Plot the posterior of model parameters\"}\naz.plot_posterior(trace);\n```\n\n::: {.cell-output .cell-output-display}\n![The posterior distribution of the model parameters.](etra_challenge_files/figure-html/fig-example-chain-posterior-output-1.png){#fig-example-chain-posterior width=2415 height=486}\n:::\n:::\n\n\n::: {#tbl-example-chain-summary .cell tbl-cap='Summary statistics for the example chain.' execution_count=19}\n``` {.python .cell-code code-summary=\"Print summary statistics of the chain\"}\naz.summary(trace)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tau</th>\n      <td>25.294</td>\n      <td>14.685</td>\n      <td>2.000</td>\n      <td>44.000</td>\n      <td>0.526</td>\n      <td>0.372</td>\n      <td>765.0</td>\n      <td>1868.0</td>\n      <td>1.01</td>\n    </tr>\n    <tr>\n      <th>lambda_1</th>\n      <td>2.056</td>\n      <td>0.398</td>\n      <td>1.356</td>\n      <td>2.789</td>\n      <td>0.015</td>\n      <td>0.011</td>\n      <td>925.0</td>\n      <td>574.0</td>\n      <td>1.01</td>\n    </tr>\n    <tr>\n      <th>lambda_2</th>\n      <td>2.005</td>\n      <td>0.808</td>\n      <td>0.409</td>\n      <td>3.146</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>3998.0</td>\n      <td>2360.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>delta</th>\n      <td>-0.051</td>\n      <td>0.912</td>\n      <td>-1.741</td>\n      <td>1.281</td>\n      <td>0.018</td>\n      <td>0.013</td>\n      <td>1871.0</td>\n      <td>2078.0</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice that the posterior distribution of $\\Delta$ is centered around 0, leading\nus to believe there is no change in the the rate of saccades. In case there was\na change, we would expect a shift either to the positive or negative side. This\nnotion is further reinforced by looking at the Highest Density Interval (HDI) of\nthe posterior distribution of $\\tau$ which basically spans the entire duration\nof the trial.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-summary=\"Calculate the probability of null hypothesis\"}\np_l2_ge_l1 = np.mean(trace.posterior[\"delta\"].values >= 0)\n```\n:::\n\n\nWe have\n\n::: {.cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nPr[H_0] = Pr[lambda_2 >= lambda_1]= 0.47\n```\n:::\n:::\n\n\nAnd as the probability is quite high, we do not reject the null hypothesis in\nthis example.\n\n### Natural scenes\n\nWe do the same analysis as above for all trials using _Natural_ scenes for\nsubject ``022``.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-summary=\"Sample the posterior distributions for all natural scenes\"}\nstimuli_natural = df_hyp1_natural_sacc.stimulus_id.unique()\nstimuli_natural.sort()\nchains_natural = []\nfor stimulus in stimuli_natural:\n    trial = df_hyp1_natural_sacc[df_hyp1_natural_sacc.stimulus_id == stimulus]\n    model = model_saccades(trial)\n    with model:\n        chain = pm.sample(5_000)\n        chains_natural.append(chain)\n```\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code code-summary=\"Combine Natural data from all chains\"}\ncombined_natural = []\nfor stimulus, chain in zip(stimuli_natural, chains_natural):\n    df = chain.posterior.to_dataframe()\n    df[\"stimulus\"] = stimulus\n    combined_natural.append(df)\n    \ncombined_natural = pd.concat(combined_natural)\ncombined_natural = combined_natural.assign(abs_delta=lambda _: np.abs(_['delta']))\n```\n:::\n\n\n@fig-natural-delta-posterior shows the posterior density estimate for $\\Delta$\nfor all _Natural_ scene trials.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-summary=\"Plot Posterior density of $\\Delta$ for all natural scenes.\"}\nstatistic = \"delta\"\n# Truncate values to 99% most plausible\nlow, up = combined_natural.delta.quantile([0.005, 0.995])\nhlp = pd.DataFrame()\nhlp[statistic] = combined_natural[statistic].where(\n    combined_natural[statistic].between(low, up)\n)\nhlp[\"stimulus\"] = combined_natural[\"stimulus\"]\nhlp.reset_index(inplace=True)\n\ng = sns.displot(data=hlp, x=statistic, hue=\"stimulus\", kind=\"kde\", aspect=2)\ng.ax.axvline(0, color='k', ls='dashed', alpha=0.5)\ng.set_xlabels(\"$\\Delta$\")\ng.set_ylabels(\"Probability\")\ng.set(title=f\"Posterior density of $\\Delta$\");\n```\n\n::: {.cell-output .cell-output-display}\n![Posterior density of $\\Delta$ for all natural scenes of one chosen subject.](etra_challenge_files/figure-html/fig-natural-delta-posterior-output-1.png){#fig-natural-delta-posterior width=1052 height=485}\n:::\n:::\n\n\nThere does seem to be some variability from scene to scene, however, from the\ngraph we can still see that _on average_ the change of the frequencies seems to\nbe centered around zero, even shifted sligtly toward the negative side meaning\nthat it is more probable that the frequency of saccades _decreases_ slightly as\ntime progresses.\n\n### Blank scenes\n\nWe now repeat the exact same procedure, but for trials with _Blank_ scene.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code code-summary=\"Sample the posterior distributions for all blank scenes\"}\ntrials_blank = df_hyp1_blank_sacc.trial.unique()\ntrials_blank.sort()\nchains_blank = []\nfor trial in trials_blank:\n    trial_data = df_hyp1_blank_sacc[df_hyp1_blank_sacc.trial == trial]\n    model = model_saccades(trial_data)\n    with model:\n        chain = pm.sample(5_000)\n        chains_blank.append(chain)\n```\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-summary=\"Combine Blank data from all chains\"}\ncombined_blank = []\nfor trial, chain in zip(trials_blank, chains_blank):\n    df = chain.posterior.to_dataframe()\n    df[\"trial\"] = trial\n    combined_blank.append(df)\n    \ncombined_blank = pd.concat(combined_blank)\ncombined_blank = combined_blank.assign(abs_delta=lambda _: np.abs(_['delta']))\n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code code-summary=\"Plot posterior density of $\\Delta$ for all blank scenes.\"}\nstatistic = \"delta\"\n# Truncate values to 99% most plausible\nlow, up = combined_blank.delta.quantile([0.005, 0.995])\nhlp = pd.DataFrame()\nhlp[statistic] = combined_blank[statistic].where(\n    combined_blank[statistic].between(low, up)\n)\nhlp[\"trial\"] = combined_blank[\"trial\"]\nhlp.reset_index(inplace=True)\n\ng = sns.displot(data=hlp, x=\"delta\", hue=\"trial\", kind=\"kde\", aspect=2)\ng.ax.axvline(0, color='k', ls='dashed', alpha=0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![Posterior density of $\\Delta$ for all blank scenes of one chosen subject.](etra_challenge_files/figure-html/fig-blank-delta-posterior-output-1.png){#fig-blank-delta-posterior width=1031 height=464}\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code code-summary=\"Compute probabilities of extreme cases\"}\np_l2_lt_l1_094 = (combined_blank[combined_blank.trial == \"094\"].delta < 0).mean()\np_l2_gt_l1_070 = (combined_blank[combined_blank.trial == \"070\"].delta > 0).mean()\n```\n:::\n\n\nThe situation here is much more diverse in this case. In\n@fig-blank-delta-posterior we can see that we are able to find trials on both\nends of the spectrum when considering the change in frequency of saccades. For\nexample, for the trial ``094`` we have\n\n::: {.cell execution_count=29}\n\n::: {.cell-output .cell-output-stdout}\n```\nPr[ lambda_2 < lambda_1 ] = 0.99995\n```\n:::\n:::\n\n\nso there is a very high probability of a _decrease_ in the frequency of saccades\nat some point in the trial.\nOn the other hand, for trial ``061`` we have \n\n::: {.cell execution_count=30}\n\n::: {.cell-output .cell-output-stdout}\n```\nPr[ lambda_2 > lambda_1 ] = 0.99985\n```\n:::\n:::\n\n\nmeaning a high probability of an _increase_ in frequency.\n\nThis seems quite surprising as one might expect consistent behaviour when\nviewing the same blank scene multiple times.\n\n## Results\n\n::: {.cell execution_count=31}\n``` {.python .cell-code code-summary=\"Compute the probability of null hypothesis being true\"}\np_l2_ge_l1_natural_combined = (combined_natural.delta >= 0).mean()\n```\n:::\n\n\nConsidering all natural scenes as a whole, the probability that the frequency of\nsaccades does not decrease is\n\n::: {.cell execution_count=32}\n\n::: {.cell-output .cell-output-stdout}\n```\nPr[ H_0 ] = Pr[ lambda_2 >= lambda_1 ] = 0.399\n```\n:::\n:::\n\n\nwhich is not low enough to reject the null hypothesis\n\n::: {.cell execution_count=33}\n``` {.python .cell-code code-summary=\"Compute probability of null hypothesis for Blank trials\"}\np_l2_ge_l1_blank_combined = (combined_blank.delta >= 0).mean()\n```\n:::\n\n\nConsidering all the blank trials as a whole, the probability of increase in\nsaccades is\n\n::: {.cell execution_count=34}\n\n::: {.cell-output .cell-output-stdout}\n```\nPr[ H_0 ] = Pr[ lambda_2 >= lambda_1 ] = 0.424\n```\n:::\n:::\n\n\nwhich is not low enough to reject the null hypothesis\n\n## Discussion\n\nIn this section, we will examine certain assumptions that may have influenced\nthe outcome of our model. One assumption made is that the distribution of\nsaccades is independent of the selected time interval for examination. However,\nit is possible that the number of saccades may vary over time due to factors\nsuch as participant fatigue.\n\nAdditionally, we have assumed that there is a single point of change within the\ndata. However, it is feasible that there may be multiple points of change or\nthat the change may occur gradually. Further exploration of these possibilities\nmay serve as a means for model improvement.\n\nFurthermore, our model utilizes specific priors for the parameters. While we\nhave chosen what we believe to be conservative priors, alternative options such\nas Half-Cauchy or Truncated Normal distributions for the $\\lambda$s should also\nbe considered. Additionally, the selection of hyperparameters should be\nevaluated. We claim that the choice of hyperparameters did not have a\nsignificant impact on the model's results and served primarily to improve the\nspeed of convergence of the chain.\n\nLastly, we did not take into account the examination of trials as a cohesive\nunit. Each trial was analyzed individually, but it is possible that the order,\ntype and difficulty of prior trials may have affected performance on subsequent\ntrials.\n\n\n# Hypothesis 2: Pupil dilation\n\nWe will study the following hypotheses:\n\n * $H_0^{(T)}$ _(null hypothesis)_ : The mean pupil size is equal when comparing\n   Blank scene and a type T scene\n * $H_1^{(T)}$ _(alternative hypothesis)_ : The mean pupil size is _not_ equal\n   when comparing Blank scene and a type T scene\n\n## Load Data\n\nAgain, we will limit our analysis to data collected from a single participant,\nspecifically participant ID ``022``. The participant was selected arbitrarily.\nWe have selected and loaded all data collected during the \"Free Viewing\" task.\nOur focus will be on the following columns of the dataset:\n\n* ``time``: representing the time in milliseconds elapsed from the beginning of\n  the trial.\n* ``rp``: indicating the size of the right pupil.\n\nIn @tbl-hyp2-data we list a sample of data we work with.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code code-summary=\"Load data\"}\ndf_hyp2 = (dataset.data_dir / \"data\" / \"{0:0>3}\".format(subject_no))\\\n    .glob(\"*FreeViewing_*.csv\")\ndf_hyp2 = pd.concat((read_data(f) for f in df_hyp2)).sort_values(by=\"Time\")\n\ndf_hyp2 = df_hyp2.rename(\n    {\n        \"Time\": \"time\",\n        \"trial_id\": \"trial\",\n        \"RP\": \"rp\"\n    },\n    axis=1\n)\ndf_hyp2[\"time\"] = df_hyp2.groupby([\"participant_id\", \"trial\"])[\"time\"]\\\n    .transform(lambda x: x - x.min())\n\ndf_hyp2 = df_hyp2[\n    [\n        'participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id',\n        'time','rp',\n    ]\n]\n```\n:::\n\n\n::: {#tbl-hyp2-data .cell tbl-cap='Sample of data for hypothesis 2' execution_count=36}\n``` {.python .cell-code code-summary=\"List a sample of data for hypothesis 2.\"}\ndf_hyp2.head() \n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>trial</th>\n      <th>fv_fixation</th>\n      <th>task_type</th>\n      <th>stimulus_id</th>\n      <th>time</th>\n      <th>rp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>0</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>2</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>4</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>6</td>\n      <td>587</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>022</td>\n      <td>001</td>\n      <td>FreeViewing</td>\n      <td>Puzzle</td>\n      <td>puz008</td>\n      <td>8</td>\n      <td>585</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Data Manipulation\n\nAgain, we remove the blinks and then calculate the mean pupil size for each\ntrial.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code code-summary=\"Remove blinks and calculate average pupil size.\"}\ndf_hyp2_avg_rp = df_hyp2.groupby(\"trial\")\\\n    .apply(remove_blinks)\\\n    .reset_index(drop=True)\\\n    .groupby([\"trial\", \"task_type\"])\\\n    .agg(avg_rp=('rp', 'mean'))\\\n    .reset_index()\\\n    .drop(\"trial\", axis=1)\n```\n:::\n\n\n::: {#tbl-avg-rp .cell tbl-cap='Average pupil size (right eye) for each task type.' execution_count=38}\n``` {.python .cell-code code-summary=\"Report the average pupil size for each task type.\"}\ndf_hyp2_avg_rp.head().round(decimals=2)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task_type</th>\n      <th>avg_rp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Puzzle</td>\n      <td>734.91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Waldo</td>\n      <td>664.90</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Natural</td>\n      <td>596.14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Blank</td>\n      <td>554.77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Waldo</td>\n      <td>657.94</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Exploration\n\nLooking at @fig-avg-rp-dist-box, we can see that there except for the \"Natural\"\nscenes there are no significant outliers in any other category. \n@tbl-avg-rp-summary lists summary statistics about the average pupil size.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code code-summary=\"Plot the distribution of the average pupil size.\"}\ng = sns.catplot(\n    data=df_hyp2_avg_rp,\n    x='task_type', y='avg_rp',\n    kind='box',\n    aspect=3\n)\ng.set_xlabels(\"Task type\")\ng.set_ylabels(\"Average pupil size\")\ng.set(title=\"Average pupil size distribution.\");\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of the average pupil size across different task types.](etra_challenge_files/figure-html/fig-avg-rp-dist-box-output-1.png){#fig-avg-rp-dist-box width=1424 height=483}\n:::\n:::\n\n\n::: {#tbl-avg-rp-summary .cell tbl-cap='Summary statistics for average pupil sizes.' execution_count=40}\n``` {.python .cell-code code-summary=\"Compute summary statistics for average pupil sizes\"}\ndf_hyp2_avg_rp.groupby(\"task_type\").describe().round(decimals=2)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"8\" halign=\"left\">avg_rp</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>task_type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Blank</th>\n      <td>15.0</td>\n      <td>589.78</td>\n      <td>89.22</td>\n      <td>438.20</td>\n      <td>537.63</td>\n      <td>592.73</td>\n      <td>650.43</td>\n      <td>711.32</td>\n    </tr>\n    <tr>\n      <th>Natural</th>\n      <td>15.0</td>\n      <td>635.84</td>\n      <td>91.14</td>\n      <td>464.97</td>\n      <td>587.33</td>\n      <td>636.75</td>\n      <td>662.73</td>\n      <td>797.62</td>\n    </tr>\n    <tr>\n      <th>Puzzle</th>\n      <td>15.0</td>\n      <td>699.44</td>\n      <td>89.15</td>\n      <td>594.43</td>\n      <td>631.53</td>\n      <td>664.14</td>\n      <td>748.29</td>\n      <td>856.61</td>\n    </tr>\n    <tr>\n      <th>Waldo</th>\n      <td>15.0</td>\n      <td>724.14</td>\n      <td>112.28</td>\n      <td>598.10</td>\n      <td>627.82</td>\n      <td>709.60</td>\n      <td>801.23</td>\n      <td>941.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet us look at the distribution of the means. If we want to use t-test, we want\nour data to be roughly normally distributed.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code code-summary=\"Plot the distribution of average pupil sizes\"}\ng = sns.displot(\n    data=df_hyp2_avg_rp,\n    x='avg_rp',\n    hue='task_type',\n    col=\"task_type\"\n)\ng.set_xlabels(\"Average pupil size\")\ng.set_ylabels(\"Count\");\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of average pupil sizes by task type.](etra_challenge_files/figure-html/fig-avg-rp-dist-bar-output-1.png){#fig-avg-rp-dist-bar width=2014 height=464}\n:::\n:::\n\n\nLooking at the graphs in @fig-avg-rp-dist-bar, the distribution of the data\nseems to be roughly normal, however it is hard to gauge this from such a small\nsample size. We use the Shapiro-Wilk test, which tests the null hypothesis that\nthe data was drawn from a normal distribution.\n\n::: {.cell execution_count=42}\n``` {.python .cell-code code-summary=\"Compute Shapiro-Wilk test for distribution normality\"}\nprint(\"Resulting p-values from the Shapiro-Wilk test:\\n\")\nfor task_type in [\"Blank\", \"Puzzle\", \"Waldo\", \"Natural\"]:\n    shapiro_result = stats.shapiro(\n        df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == task_type].avg_rp\n    )\n    print(\"\\t{}\\t: {:0.3}\".format(task_type, shapiro_result.pvalue))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulting p-values from the Shapiro-Wilk test:\n\n\tBlank\t: 0.374\n\tPuzzle\t: 0.0556\n\tWaldo\t: 0.142\n\tNatural\t: 0.592\n```\n:::\n:::\n\n\nUsing signifance level of $\\alpha = 0.05$, we conclude that the data is\ndistributed normally (although this is a bit debatable in the case of the\n\"Puzzle\" task).\n\n## Results\n\nGiven that we are evaluating the same participant across different tasks, it is\nappropriate to use the paired t-test as the data points are linked to one\nanother. This statistical test allows for the comparison of each task with our\nchosen baseline, which is the blank scene.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code code-summary=\"Compute paired t-test for each category with the baseline.\"}\nbaseline = \"Blank\"\n\nprint(\"Resulting p-values from the paired t-test:\\n\")\nfor task_type in [\"Puzzle\", \"Waldo\", \"Natural\"]:\n    ttest_result = stats.ttest_ind(\n        df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == baseline].avg_rp,\n        df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == task_type].avg_rp,\n    )\n    print(\"\\t{}\\t: {:0.3}\".format(task_type, ttest_result.pvalue))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResulting p-values from the paired t-test:\n\n\tPuzzle\t: 0.00222\n\tWaldo\t: 0.00113\n\tNatural\t: 0.173\n```\n:::\n:::\n\n\nThus using significance level of $\\alpha = 0.05$, we reject the null hypothesis\nfor the _Puzzle_ and _Waldo_ tasks, meaning the mean pupil size differs from the\nbaseline (greater as can be seen from @fig-avg-rp-dist-box). We were not able to\nreject the null hypothesis in case of the _Natural_ task. \n\n## Discussion\n\nAnother interesting metric to consider might be the number of times the pupil\nsize peaks and the count of such peaks.\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "etra_challenge_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}