[
  {
    "objectID": "etra_challenge.html",
    "href": "etra_challenge.html",
    "title": "ETRA Challenge Report",
    "section": "",
    "text": "In 2019, the ETRA organization announced a challenge for the analysis of a dataset pertaining to human eye-movement. The objective of the challenge was to utilize various tools to uncover intriguing relationships and information within the data, potentially leading to novel insights. Additional details regarding the challenge can be found in the official Challenge Track Call for Papers issued by ETRA."
  },
  {
    "objectID": "etra_challenge.html#load-data",
    "href": "etra_challenge.html#load-data",
    "title": "ETRA Challenge Report",
    "section": "Load Data",
    "text": "Load Data\nWe will restrict ourselves only to one subject. We have arbitrarily chosen the participant with ID 022. We select and load all Free Viewing data with Natural and Blank scenes. We will be interested in columns:\n\ntime : Indicating the time in milliseconds from the beggining of the trail.\nx, y : The position on the screen where the participant was looking at the given time.\nrp: Pupil size of the right eye.\n\nIn Table 1 we provide a sample of the data we will use.\n\n\nLoad data\ndf_hyp1 = (dataset.data_dir / \"data\" / \"{0:0>3}\".format(subject_no)).glob(\"*FreeViewing_*.csv\")\ndf_hyp1 = pd.concat((read_data(f) for f in df_hyp1)).sort_values(by=\"Time\")\n\ndf_hyp1 = df_hyp1.rename({\"Time\": \"time\", \"trial_id\": \"trial\", \"LXpix\": \"x\", \"LYpix\": \"y\", \"RP\": 'rp'}, axis=1)\ndf_hyp1[\"time\"] = df_hyp1.groupby([\"participant_id\", \"trial\"])[\"time\"].transform(lambda x: x - x.min())\ndf_hyp1 = df_hyp1[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id',\n       'time', 'x', 'y', 'rp']]\n\n\n\n\nPrint sample of hypothesis 1 data\ndf_hyp1.head()\n\n\n\n\n\n\nTable 1:  Sample of the data for hypothesis 1. \n  \n    \n      \n      participant_id\n      trial\n      fv_fixation\n      task_type\n      stimulus_id\n      time\n      x\n      y\n      rp\n    \n  \n  \n    \n      0\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      0\n      454.42\n      317.175\n      585\n    \n    \n      1\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      2\n      454.66\n      312.000\n      585\n    \n    \n      2\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      4\n      453.30\n      312.375\n      585\n    \n    \n      3\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      6\n      454.10\n      309.450\n      587\n    \n    \n      4\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      8\n      454.82\n      310.725\n      585"
  },
  {
    "objectID": "etra_challenge.html#data-manipulation",
    "href": "etra_challenge.html#data-manipulation",
    "title": "ETRA Challenge Report",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nFirst step of the pre-processing will be to remove the blinks. We look at the pupil size and consider every value smaller than some threshold to be an indication of a blink. The average pupil size to be between 1 and 8 mm (“Pupil Diameter,” n.d.). We set 1 mm as our threshold. We will then discard a portion of our data around such points. The average blink duration ranges between 100 and 400 ms (“Average Duration of a Single Eye Blink,” n.d.). We delete another 200 ms before and after each blink/semi-blink to eliminate the initial and final parts in which the pupil was still partially occluded.\n\n\nFunction for removing blinks\ndef remove_blinks(data : pd.DataFrame, column='rp'):\n    ans = data.copy()\n    n_rows = ans.shape[0]\n    \n    lows = np.where(ans['rp'].lt(100))[0]\n    for low in lows:\n        ans.loc[max(0, low-40):min(low+40, n_rows), column] = pd.NA\n        \n    return ans.dropna()\n\n\n\n\nPlot pupil size with and without blinks\nbefore = df_hyp1[df_hyp1.trial == '106'].assign(blinks_removed='No')\nafter = remove_blinks(before).assign(blinks_removed='Yes')\nhlp = pd.concat([before, after])\n\ng = sns.relplot(data=hlp, x='time', y='rp', kind='line', row='blinks_removed', aspect=3)\ng.set_xlabels(\"Time (ms)\")\ng.set_ylabels(\"Pupil size\")\ng.axes[0, 0].set_title(\"Pupil size as measured\")\ng.axes[1, 0].set_title(\"Pupil size with blinks removed\");\n\n\n\n\n\nFigure 1: Pupil size data with blinks (top) and with blinks removed (bottom).\n\n\n\n\nFor each trial, we will detect saccades using the REMoDNaV python library (Dar, Wagner, and Hanke 2019). This gives us the start and end times of saccades (and other events) as well as additional information such as start and end position of the gaze or peak velocity. We will consider only the start times of the saccades and disregard all other data. Table 2 lists a sample of the processed data.\n\n\nFunction for detecting saccades\nfrom etra import detect\n\ndef detect_saccades_by_groups(data, groupby=[\"participant_id\", \"trial\"]):\n    ans = []\n    groups = data.groupby(groupby)\n    for (pid, trial), group in groups:\n        tmp = remove_blinks(group)\n        tmp = detect(group)\n        tmp = tmp[tmp[\"label\"] == \"SACC\"]\n        tmp[\"participant_id\"] = pid\n        tmp[\"trial\"] = trial\n        ans.append(tmp)\n    \n    return pd.concat(ans)\n\n\n\n\nPreprocess data\ndf_hyp1_natural = df_hyp1[df_hyp1.task_type == \"Natural\"]\ndf_hyp1_natural_sacc = detect_saccades_by_groups(\n    df_hyp1_natural\n)\n\ndf_hyp1_blank = df_hyp1[df_hyp1.task_type == \"Blank\"]\ndf_hyp1_blank_sacc = detect_saccades_by_groups(\n    df_hyp1_blank\n)\n\ntrial_info_natural = df_hyp1_natural[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id']].drop_duplicates()\ndf_hyp1_natural_sacc = trial_info_natural.merge(df_hyp1_natural_sacc, on=[\n    \"participant_id\", \"trial\"], how=\"left\")\ndf_hyp1_natural_sacc = df_hyp1_natural_sacc[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id', 'start_time']]\n\ntrial_info_blank = df_hyp1_blank[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id']].drop_duplicates()\ndf_hyp1_blank_sacc = trial_info_blank.merge(df_hyp1_blank_sacc, on=[\n    \"participant_id\", \"trial\"], how=\"left\")\ndf_hyp1_blank_sacc = df_hyp1_blank_sacc[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id', 'start_time']]\n\n\n\n\nPrint sample of processed data\ndf_hyp1_natural_sacc.head()\n\n\n\n\n\n\nTable 2:  Sample of Freeviewing data. \n  \n    \n      \n      participant_id\n      trial\n      fv_fixation\n      task_type\n      stimulus_id\n      start_time\n    \n  \n  \n    \n      0\n      022\n      003\n      FreeViewing\n      Natural\n      nat010\n      258\n    \n    \n      1\n      022\n      003\n      FreeViewing\n      Natural\n      nat010\n      480\n    \n    \n      2\n      022\n      003\n      FreeViewing\n      Natural\n      nat010\n      670\n    \n    \n      3\n      022\n      003\n      FreeViewing\n      Natural\n      nat010\n      1166\n    \n    \n      4\n      022\n      003\n      FreeViewing\n      Natural\n      nat010\n      1266"
  },
  {
    "objectID": "etra_challenge.html#methodology-and-exploration",
    "href": "etra_challenge.html#methodology-and-exploration",
    "title": "ETRA Challenge Report",
    "section": "Methodology and Exploration",
    "text": "Methodology and Exploration\nTo help us understand how we should model the observations, we first look at the distribution of saccades in time. The \\(x\\) axis represents the time from the start of a trial and each row is a sequence of dots representing the start time of a saccade.\n\n\nPlot the distriution of saccades in time\ng = sns.catplot(\n    data=df_hyp1_natural_sacc,\n    x='start_time',\n    y='stimulus_id',\n    jitter=False,\n    aspect=3,\n)\ng.set_xlabels(\"Start time of a saccade in milliseconds\")\ng.set_ylabels(\"ID of the stimulus\")\ng.set(title=\"Distribution of saccades in time\");\n\n\n\n\n\nFigure 2: Distribution of saccades in time in Natural scenes for one participant.\n\n\n\n\nAs the occurance of a saccade is a complex process that depends on the scene viewed, mental state of the participant, tiredness or maybe even things like personal history, eyesight, mood and many other factors, it is impossible for us to accurately predict the occurance of the next saccade. To simplify the situation, we might look at it as on a stochastic process, where for some small enough time interval, we have a fixed probability for an occurance of a saccade.\nLet \\(N_t\\) be a variable counting the number of saccades in a time interval \\([0,t]\\). Our assumption can be formulated as \\[\n\\lim_{t \\to 0} \\Pr[N_t \\ge 1]/t = \\lambda\n\\] for some \\(\\lambda \\in \\mathbb{R}^+\\). That is, the probability of an saccade in a small time interval \\([0, t]\\) tends to \\(\\lambda t\\).\nDue to physical human limitations, we also have \\[\n\\lim_{t \\to 0} \\Pr[N_t \\ge 2]/t = 0\n\\] or said differently, for a small enough time interval, it is impossible to have two or more successive saccades.\nThe number of saccades in some interval \\([t, s]\\) can be expressed then as \\(N(t) - N(s)\\). Additionally, we will assume that for any two disjoint time intervals \\([t_1, t_2]\\) and \\([t_3, t_4]\\), the distribution of \\(N(t_2) - N(t_1)\\) is independent of the distribution of \\(N(t_4) - N(t_3)\\).\nWith these assumtions in place, we are justified to model the occurances of saccades as a Poisson process with rate \\(\\lambda\\) (Mitzenmacher and Upfal 2012).\n\n\nFunction for counting the number of saccades each second\ndef saccade_frequency_count_by_second(data):\n    \"\"\" Count the number of saccades in each second of the trial \"\"\"\n    \n    bins = (data.start_time // 1000).value_counts().sort_index()\n    data = pd.DataFrame({\"time\": range(45)})\n    data[\"sacc_count\"] = bins\n    data.fillna(0, inplace=True)\n    data.sacc_count = data.sacc_count.astype(int)\n    \n    return data\n\n\nThe theory of Poisson processes then tells us that the number of saccades in some time period of length \\(t\\) is a Poisson random variable with expectation \\(\\lambda t\\). We will divide the time of the experiment to seconds (so \\(t = 1\\)) and count the number of saccades in each second. The Figure 3 illustrates such counts for one chosen trial.\n\n\nChose one scene as an example\nstimulus = \"nat013\"\nscene = df_hyp1_natural_sacc[df_hyp1_natural_sacc.stimulus_id == stimulus]\n\n\n\n\nPlot the frequency of saccades in time\nhlp = scene.copy()\nhlp.start_time = hlp.start_time.apply(lambda ms: ms // 1000)\n\ng = sns.displot(data=hlp, x='start_time', bins=45, stat=\"count\", aspect=3)\ng.set_xlabels(\"Time (s)\")\ng.set_ylabels(\"Number of saccades\")\ng.set(title=f\"Frequency of saccades\");\n\n\n\n\n\nFigure 3: Frequency of saccades in time for one chosen natural scene.\n\n\n\n\nWe are looking for a change in behaviour in our participants. To model it we will assume that in some initial time interval the number of saccades will follow some Poisson process with rate \\(\\lambda_1\\) up until some time \\(\\tau\\) after which something changes and the saccades will follow a different process with rate \\(\\lambda_2\\).\nLet \\(S_t\\) denote the count of saccades at time \\(t\\). We have \\[\n    S_t \\sim \\mathrm{Poisson}(\\lambda^{(t)})\n\\] However, we do not know what the parameter \\(\\lambda^{(t)}\\) is. It might be either \\(\\lambda_1\\) or \\(\\lambda_2\\) depending on where we put the changing point \\(\\tau\\), so \\[\n    \\lambda^{(t)} = \\begin{cases}\n        \\lambda_1 && t < \\tau \\\\\n        \\lambda_2 && t \\ge \\tau\n    \\end{cases}\n\\]\nWe will try to infer these variables from our data using Bayesian statistics. We will need to choose prior distributions to do that. For \\(\\lambda_1\\) and \\(\\lambda_2\\) we need to choose from continouous distribution that is able to obtain (potentially) all positive values. Exponential distribution satisfies this requirement, so we set \\[\n\\begin{align*}\n    \\lambda_1 \\sim \\mathrm{Exp}(\\alpha)\\\\\n    \\lambda_2 \\sim \\mathrm{Exp}(\\alpha)\n\\end{align*}\n\\] where \\(\\alpha\\) is some hyper-parameter. As we have no prior knowledge about \\(\\tau\\), we simply choose \\[\n    \\tau \\sim \\mathrm{Unif}(0, 44)\n\\]\nTo help with the inference, we set the hyperparameter \\(\\alpha\\) such that \\[\n    \\frac{1}{\\alpha}\n    = \\mathbb{E}[\\lambda \\mid \\alpha]\n    \\approx \\frac{1}{45} \\sum_{t = 0}^{44} S_t\n\\] which is about the value we would expect the \\(\\lambda\\)s to be distributed around.\n\n\n\n\n\n\nNote\n\n\n\nThis approach was adapted from the first chapter of Bayesian Methods for Hackers.\n\n\nFor our modelling we will use the PyMC (Salvatier, Wiecki, and Fonnesbeck 2016) implementation of Monte Carlo Markov Chains, using the NUTS sampler with default parameters to generate \\(5000\\) samples from the posterior distribution.\n\n\nDefine model for the saccade frequency\ndef model_saccades(data):\n    # Get the count data for one chosen trial\n    count_data = saccade_frequency_count_by_second(data)\n    count_data = count_data.sacc_count\n    n_count_data = count_data.shape[0]\n    \n    with pm.Model() as model:\n        alpha = 1.0/count_data.mean()\n\n        lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n        lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n\n        tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data - 1)\n\n        index = np.arange(n_count_data)\n        lambda_ = pm.math.switch( tau >= index, lambda_1, lambda_2 )\n        \n        delta = pm.Deterministic(\"delta\", lambda_2 - lambda_1)\n\n        observation = pm.Poisson(\"obs\", lambda_, observed=count_data)\n        \n    return model\n\n\n\n\nRun the chain\nmodel = model_saccades(scene)\nwith model:\n    trace = pm.sample(5_000)\n\n\nFigure 4 shows the posterior distributions of the parameters \\(\\tau\\), \\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\Delta = \\lambda_2 - \\lambda_1\\), while Table 3 provides summary statistics from the chain.\n\n\nPlot the posterior of model parameters\naz.plot_posterior(trace);\n\n\n\n\n\nFigure 4: The posterior distribution of the model parameters.\n\n\n\n\n\n\nPrint summary statistics of the chain\naz.summary(trace)\n\n\n\n\n\n\nTable 3:  Summary statistics for the example chain. \n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      tau\n      23.412\n      14.961\n      1.000\n      44.000\n      0.478\n      0.343\n      925.0\n      1898.0\n      1.0\n    \n    \n      lambda_1\n      2.072\n      0.514\n      1.230\n      3.034\n      0.014\n      0.010\n      2244.0\n      1621.0\n      1.0\n    \n    \n      lambda_2\n      1.993\n      0.739\n      0.493\n      3.054\n      0.014\n      0.010\n      3253.0\n      1812.0\n      1.0\n    \n    \n      delta\n      -0.079\n      0.913\n      -1.765\n      1.312\n      0.019\n      0.013\n      2414.0\n      2124.0\n      1.0\n    \n  \n\n\n\n\n\nNotice that the posterior distribution of \\(\\Delta\\) is centered around 0, leading us to believe there is no change in the the rate of saccades. In case there was a change, we would expect a shift either to the positive or negative side. This notion is further reinforced by looking at the Highest Density Interval (HDI) of the posterior distribution of \\(\\tau\\) which basically spans the entire duration of the trial.\n\n\nCalculate the probability of null hypothesis\np_l2_ge_l1 = np.mean(trace.posterior[\"delta\"].values >= 0)\n\n\nWe have\n\n\nPr[H_0] = Pr[lambda_2 >= lambda_1]= 0.47\n\n\nAnd as the probability is quite high, we do not reject the null hypothesis in this example.\n\nNatural scenes\nWe do the same analysis as above for all trials using Natural scenes for subject 022.\n\n\nSample the posterior distributions for all natural scenes\nstimuli_natural = df_hyp1_natural_sacc.stimulus_id.unique()\nstimuli_natural.sort()\nchains_natural = []\nfor stimulus in stimuli_natural:\n    trial = df_hyp1_natural_sacc[df_hyp1_natural_sacc.stimulus_id == stimulus]\n    model = model_saccades(trial)\n    with model:\n        chain = pm.sample(5_000)\n        chains_natural.append(chain)\n\n\n\n\nCombine Natural data from all chains\ncombined_natural = []\nfor stimulus, chain in zip(stimuli_natural, chains_natural):\n    df = chain.posterior.to_dataframe()\n    df[\"stimulus\"] = stimulus\n    combined_natural.append(df)\n    \ncombined_natural = pd.concat(combined_natural)\ncombined_natural = combined_natural.assign(abs_delta=lambda _: np.abs(_['delta']))\n\n\nFigure 5 shows the posterior density estimate for \\(\\Delta\\) for all Natural scene trials.\n\n\nPlot Posterior density of \\(\\Delta\\) for all natural scenes.\nstatistic = \"delta\"\n# Truncate values to 99% most plausible\nlow, up = combined_natural.delta.quantile([0.005, 0.995])\nhlp = pd.DataFrame()\nhlp[statistic] = combined_natural[statistic].where(combined_natural[statistic].between(low, up))\nhlp[\"stimulus\"] = combined_natural[\"stimulus\"]\nhlp.reset_index(inplace=True)\n\ng = sns.displot(data=hlp, x=statistic, hue=\"stimulus\", kind=\"kde\", aspect=2)\ng.ax.axvline(0, color='k', ls='dashed', alpha=0.5)\ng.set_xlabels(\"$\\Delta$\")\ng.set_ylabels(\"Probability\")\ng.set(title=f\"Posterior density of $\\Delta$\");\n\n\n\n\n\nFigure 5: Posterior density of \\(\\Delta\\) for all natural scenes of one chosen subject.\n\n\n\n\nThere does seem to be some variability from scene to scene, however, from the graph we can still see that on average the change of the frequencies seems to be centered around zero, even shifted sligtly toward the negative side meaning that it is more probable that the frequency of saccades decreases slightly as time progresses.\n\n\nBlank scenes\nWe now repeat the exact same procedure, but for trials with Blank scene.\n\n\nSample the posterior distributions for all blank scenes\ntrials_blank = df_hyp1_blank_sacc.trial.unique()\ntrials_blank.sort()\nchains_blank = []\nfor trial in trials_blank:\n    trial_data = df_hyp1_blank_sacc[df_hyp1_blank_sacc.trial == trial]\n    model = model_saccades(trial_data)\n    with model:\n        chain = pm.sample(5_000)\n        chains_blank.append(chain)\n\n\n\n\nCombine Blank data from all chains\ncombined_blank = []\nfor trial, chain in zip(trials_blank, chains_blank):\n    df = chain.posterior.to_dataframe()\n    df[\"trial\"] = trial\n    combined_blank.append(df)\n    \ncombined_blank = pd.concat(combined_blank)\ncombined_blank = combined_blank.assign(abs_delta=lambda _: np.abs(_['delta']))\n\n\n\n\nPlot posterior density of \\(\\Delta\\) for all blank scenes.\nstatistic = \"delta\"\n# Truncate values to 99% most plausible\nlow, up = combined_blank.delta.quantile([0.005, 0.995])\nhlp = pd.DataFrame()\nhlp[statistic] = combined_blank[statistic].where(combined_blank[statistic].between(low, up))\nhlp[\"trial\"] = combined_blank[\"trial\"]\nhlp.reset_index(inplace=True)\n\ng = sns.displot(data=hlp, x=\"delta\", hue=\"trial\", kind=\"kde\", aspect=2)\ng.ax.axvline(0, color='k', ls='dashed', alpha=0.5);\n\n\n\n\n\nFigure 6: Posterior density of \\(\\Delta\\) for all blank scenes of one chosen subject.\n\n\n\n\n\n\nCompute probabilities of extreme cases\np_l2_lt_l1_094 = (combined_blank[combined_blank.trial == \"094\"].delta < 0).mean()\np_l2_gt_l1_070 = (combined_blank[combined_blank.trial == \"070\"].delta > 0).mean()\n\n\nThe situation here is much more diverse in this case. We can see that we can find trials on both ends of the spectrum when considering the change in frequency of saccades. For example, for the trial 094 we have\n\n\nPr[ lambda_2 < lambda_1 ] = 0.9917\n\n\nso there is a very high probability of a decrease in the frequency of saccades at some point in the trial. On the other hand, for trial 061 we have\n\n\nPr[ lambda_2 > lambda_1 ] = 0.99975\n\n\nmeaning a high ptobability of an increase in frequency.\nThis seems quite surprising as one might expect consistent behaviour when viewing the same blank scene multiple times."
  },
  {
    "objectID": "etra_challenge.html#results",
    "href": "etra_challenge.html#results",
    "title": "ETRA Challenge Report",
    "section": "Results",
    "text": "Results\n\n\nCompute the probability of null hypothesis being true\np_l2_ge_l1_natural_combined = (combined_natural.delta >= 0).mean()\n\n\nConsidering all natural scenes as a whole, the probability that the frequency of saccades does not decrease is\n\n\nPr[ H_0 ] = Pr[ lambda_2 >= lambda_1 ] = 0.393\n\n\nwhich is not low enough to reject the null hypothesis\n\n\nCompute probability of null hypothesis for Blank trials\np_l2_ge_l1_blank_combined = (combined_blank.delta >= 0).mean()\n\n\nConsidering all the blank trials as a whole, the probability of increase in saccades is\n\n\nPr[ H_0 ] = Pr[ lambda_2 >= lambda_1 ] = 0.421\n\n\nwhich is not low enough to reject the null hypothesis"
  },
  {
    "objectID": "etra_challenge.html#discussion",
    "href": "etra_challenge.html#discussion",
    "title": "ETRA Challenge Report",
    "section": "Discussion",
    "text": "Discussion\nIn this section, we will examine certain assumptions that may have influenced the outcome of our model. One assumption made is that the distribution of saccades is independent of the selected time interval for examination. However, it is possible that the number of saccades may vary over time due to factors such as participant fatigue.\nAdditionally, we have assumed that there is a single point of change within the data. However, it is feasible that there may be multiple points of change or that the change may occur gradually. Further exploration of these possibilities may serve as a means for model improvement.\nFurthermore, our model utilizes specific priors for the parameters. While we have chosen what we believe to be conservative priors, alternative options such as Half-Cauchy or Truncated Normal distributions for the \\(\\lambda\\)s should also be considered. Additionally, the selection of hyperparameters should be evaluated. We claim that the choice of hyperparameters did not have a significant impact on the model’s results and served primarily to improve the speed of convergence of the chain.\nLastly, we did not take into account the examination of trials as a cohesive unit. Each trial was analyzed individually, but it is possible that the order, type and difficulty of prior trials may have affected performance on subsequent trials."
  },
  {
    "objectID": "etra_challenge.html#load-data-1",
    "href": "etra_challenge.html#load-data-1",
    "title": "ETRA Challenge Report",
    "section": "Load Data",
    "text": "Load Data\nAgain, we will limit our analysis to data collected from a single participant, specifically participant ID 022. The participant was selected arbitrarily. We have selected and loaded all data collected during the “Free Viewing” task. Our focus will be on the following columns of the dataset:\n\ntime: representing the time in milliseconds elapsed from the beginning of the trial.\nrp: indicating the size of the right pupil.\n\nIn Table 4 we list a sample of data we work with.\n\n\nLoad data\ndf_hyp2 = (dataset.data_dir / \"data\" / \"{0:0>3}\".format(subject_no)).glob(\"*FreeViewing_*.csv\")\ndf_hyp2 = pd.concat((read_data(f) for f in df_hyp2)).sort_values(by=\"Time\")\n\ndf_hyp2 = df_hyp2.rename({\"Time\": \"time\", \"trial_id\": \"trial\", \"RP\": \"rp\"}, axis=1)\ndf_hyp2[\"time\"] = df_hyp2.groupby([\"participant_id\", \"trial\"])[\"time\"].transform(lambda x: x - x.min())\n\ndf_hyp2 = df_hyp2[['participant_id', 'trial', 'fv_fixation', 'task_type', 'stimulus_id',\n       'time','rp']]\n\n\n\n\nList a sample of data for hypothesis 2.\ndf_hyp2.head() \n\n\n\n\n\n\nTable 4:  Sample of data for hypothesis 2 \n  \n    \n      \n      participant_id\n      trial\n      fv_fixation\n      task_type\n      stimulus_id\n      time\n      rp\n    \n  \n  \n    \n      0\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      0\n      585\n    \n    \n      1\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      2\n      585\n    \n    \n      2\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      4\n      585\n    \n    \n      3\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      6\n      587\n    \n    \n      4\n      022\n      001\n      FreeViewing\n      Puzzle\n      puz008\n      8\n      585"
  },
  {
    "objectID": "etra_challenge.html#data-manipulation-1",
    "href": "etra_challenge.html#data-manipulation-1",
    "title": "ETRA Challenge Report",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nAgain, we remove the blinks and then calculate the mean pupil size for each trial.\n\n\nRemove blinks and calculate average pupil size.\ndf_hyp2_avg_rp = df_hyp2.groupby(\"trial\")\\\n    .apply(remove_blinks)\\\n    .reset_index(drop=True)\\\n    .groupby([\"trial\", \"task_type\"])\\\n    .agg(avg_rp=('rp', 'mean'))\\\n    .reset_index()\\\n    .drop(\"trial\", axis=1)\n\n\n\n\nReport the average pupil size for each task type.\ndf_hyp2_avg_rp.head().round(decimals=2)\n\n\n\n\n\n\nTable 5:  Average pupil size (right eye) for each task type. \n  \n    \n      \n      task_type\n      avg_rp\n    \n  \n  \n    \n      0\n      Puzzle\n      734.91\n    \n    \n      1\n      Waldo\n      664.90\n    \n    \n      2\n      Natural\n      596.14\n    \n    \n      3\n      Blank\n      554.77\n    \n    \n      4\n      Waldo\n      657.94"
  },
  {
    "objectID": "etra_challenge.html#exploration",
    "href": "etra_challenge.html#exploration",
    "title": "ETRA Challenge Report",
    "section": "Exploration",
    "text": "Exploration\nLooking at Figure 7, we can see that there except for the “Natural” scenes there are no significant outliers in any other category. Table 6 lists summary statistics about the average pupil size.\n\n\nPlot the distribution of the average pupil size.\ng = sns.catplot(data=df_hyp2_avg_rp, x='task_type', y='avg_rp', kind='box', aspect=3);\ng.set_xlabels(\"Task type\")\ng.set_ylabels(\"Average pupil size\")\ng.set(title=\"Average pupil size distribution.\");\n\n\n\n\n\nFigure 7: Distribution of the average pupil size across different task types.\n\n\n\n\n\n\nCompute summary statistics for average pupil sizes\ndf_hyp2_avg_rp.groupby(\"task_type\").describe().round(decimals=2)\n\n\n\n\n\n\nTable 6:  Summary statistics for average pupil sizes. \n  \n    \n      \n      avg_rp\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      task_type\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Blank\n      15.0\n      589.78\n      89.22\n      438.20\n      537.63\n      592.73\n      650.43\n      711.32\n    \n    \n      Natural\n      15.0\n      635.84\n      91.14\n      464.97\n      587.33\n      636.75\n      662.73\n      797.62\n    \n    \n      Puzzle\n      15.0\n      699.44\n      89.15\n      594.43\n      631.53\n      664.14\n      748.29\n      856.61\n    \n    \n      Waldo\n      15.0\n      724.14\n      112.28\n      598.10\n      627.82\n      709.60\n      801.23\n      941.75\n    \n  \n\n\n\n\n\nLet us look at the distribution of the means. If we want to use t-test, we want our data to be roughly normally distributed.\n\n\nPlot the distribution of average pupil sizes\ng = sns.displot(data=df_hyp2_avg_rp, x='avg_rp', hue='task_type', col=\"task_type\")\ng.set_xlabels(\"Average pupil size\")\ng.set_ylabels(\"Count\");\n\n\n\n\n\nFigure 8: Distribution of average pupil sizes by task type.\n\n\n\n\nLooking at the graphs in Figure 8, the distribution of the data seems to be roughly normal, however it is hard to gauge this from such a small sample size. We use the Shapiro-Wilk test, which tests the null hypothesis that the data was drawn from a normal distribution.\n\n\nCompute Shapiro-Wilk test for distribution normality\nprint(\"Resulting p-values from the Shapiro-Wilk test:\\n\")\nfor task_type in [\"Blank\", \"Puzzle\", \"Waldo\", \"Natural\"]:\n    shapiro_result = stats.shapiro(df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == task_type].avg_rp)\n    print(\"\\t{}\\t: {:0.3}\".format(task_type, shapiro_result.pvalue))\n\n\nResulting p-values from the Shapiro-Wilk test:\n\n    Blank   : 0.374\n    Puzzle  : 0.0556\n    Waldo   : 0.142\n    Natural : 0.592\n\n\nUsing signifance level of \\(\\alpha = 0.05\\), we conclude that the data is distributed normally (although this is a bit debatable in the case of the “Puzzle” task)."
  },
  {
    "objectID": "etra_challenge.html#results-1",
    "href": "etra_challenge.html#results-1",
    "title": "ETRA Challenge Report",
    "section": "Results",
    "text": "Results\nGiven that we are evaluating the same participant across different tasks, it is appropriate to use the paired t-test as the data points are linked to one another. This statistical test allows for the comparison of each task with our chosen baseline, which is the blank scene.\n\n\nCompute paired t-test for each category with the baseline.\nbaseline = \"Blank\"\n\nprint(\"Resulting p-values from the paired t-test:\\n\")\nfor task_type in [\"Puzzle\", \"Waldo\", \"Natural\"]:\n    ttest_result = stats.ttest_ind(\n        df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == baseline].avg_rp,\n        df_hyp2_avg_rp[df_hyp2_avg_rp.task_type == task_type].avg_rp,\n    )\n    print(\"\\t{}\\t: {:0.3}\".format(task_type, ttest_result.pvalue))\n\n\nResulting p-values from the paired t-test:\n\n    Puzzle  : 0.00222\n    Waldo   : 0.00113\n    Natural : 0.173\n\n\nThus using significance level of \\(\\alpha = 0.05\\), we reject the null hypothesis for the Puzzle and Waldo tasks, meaning the mean pupil size differs from the baseline (greater as can be seen from Figure 7). We were not able to reject the null hypothesis in case of the Natural task."
  },
  {
    "objectID": "etra_challenge.html#discussion-1",
    "href": "etra_challenge.html#discussion-1",
    "title": "ETRA Challenge Report",
    "section": "Discussion",
    "text": "Discussion\nAnother interesting metric to consider might be the number of times the pupil size peaks and the count of such peaks."
  }
]